---
title: "VoteMatch - Modeling"
output: html_notebook
---

### Load Data

We loaded the pre-processed data saved from the previous step.
The initial dataset contained a total of 15,484 rows, with several variables related to voters' characteristics and political opinions.

```{r}
library(dplyr)
library(nnet)
library(randomForest)
load("preprocessed_data.RData")

# Check the structure of the main data
colnames(ces_Modeling)
```
```{r}
#str(ces_Modeling)
summary(ces_Modeling)
```
### Handling Missing Value

*imm_duration*: This variable was removed due to significant missing data. It contained values resembling years of immigration, which and had a high proportion of missing values (over 13000).

*Duration__in_seconds_*: This variable was used for data quality checks and was not needed in the final model. It was removed to ensure the dataset was more focused on the relevant features for prediction.

*cps21_rel_imp*: Although the exact reason for the variable's importance is unclear, it was removed because its NA values had a high occurrence and were second only to immduration in terms of missingness. Additionally, its missing data did not align well with other variables, and its inclusion was not deemed essential for the prediction task.

```{r}
ces_Modeling_removed <- ces_Modeling %>%
  select(-imm_duration, -cps21_rel_imp, -Duration__in_seconds_)

# Remove rows with NA values in any of the features or target
ces_Modeling_no_NA <- ces_Modeling_removed %>%
  na.omit()
```

Rows with NA values in key variables were filtered out. After handling missing values and removing unnecessary variables, the data was reduced to approximately 11,000 rows.

### Data Split

The dataset was split into two groups:

- *train_data*: Training set (70%), used to train the model.
- *test_data*: Testing set (30%), used to evaluate the model's performance.

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(ces_Modeling_no_NA)), size = 0.7 * nrow(ces_Modeling_no_NA))
train_data <- ces_Modeling_no_NA[train_index, ]
test_data <- ces_Modeling_no_NA[-train_index, ]
```

### Model Selection

This project aimed to predict party vote choice using two models:

- Multinomial Logistic Regression
- Random Forest Classifier

The models were trained on the dataset, with cross-validation used for performance evaluation, and subsequently tested on a separate test dataset.

```{r}
# Multinomial Logistic Regression
log_model_1 <- multinom(votechoice ~ cps21_fed_gov_sat + pes21_province + pes21_inequal + pes21_abort2 +
                        cps21_bornin_canada + cps21_marital + cps21_age,
                        data = train_data, verbose = FALSE)

# Random Forest
rf_model_1 <- randomForest(votechoice ~ cps21_fed_gov_sat + pes21_province + pes21_inequal + pes21_abort2 +
                        cps21_bornin_canada + cps21_marital + cps21_age,
                        data = train_data)
```

### Evaluation of Model Performance

```{r}
library(ggplot2)
library(lattice)
library(caret)

# Use Multinomial Logistic Regression to predict
log_predictions_1 <- predict(log_model_1, newdata = test_data)

conf_matrix_log <- confusionMatrix(log_predictions_1, test_data$votechoice)


# Use Random Forest to predict
rf_predictions_1 <- predict(rf_model_1, newdata = test_data)

conf_matrix_rf <- confusionMatrix(rf_predictions_1, test_data$votechoice)

# Print Result
cat("\n---", "log_predictions_1", "---\n")
print(conf_matrix_log)
cat("\n---", "rf_predictions_1", "---\n")
print(conf_matrix_rf)
```

The models faced difficulties in predicting smaller classes like Green Party, which had very few samples compared to larger classes like Liberal Party and Conservative Party.

Based on the confusion matrix and sensitivity values, it is clear that the model needs adjustments. Adjusting class weights is one potential solution to improve predictions for the minority classes.

### Model Adjustments

```{r results = 'hide'}
# Define class weights (assign higher weights to minority classes)
class_weights <- c('Bloc Québécois' = 1, 
                   'Conservative Party' = 1, 
                   'Green Party' = 2,   # Higher weight for the minority class
                   'Liberal Party' = 1, 
                   'NDP' = 1, 
                   'People\'s Party' = 2)  # Higher weight for the minority class

# Train random forest model with class weights
rf_model_2 <- randomForest(votechoice ~ cps21_fed_gov_sat + pes21_province + pes21_inequal + pes21_abort2 + 
                         cps21_bornin_canada + cps21_marital + cps21_age, 
                         data = train_data, 
                         classwt = class_weights)


# Define class weights for Multinomial Logistic Regression
train_data_2<-train_data
train_data_2$weights <- ifelse(train_data$votechoice == "Bloc Québécois", 1,
                             ifelse(train_data$votechoice == "Conservative Party", 1,
                                    ifelse(train_data$votechoice == "Green Party", 2,
                                           ifelse(train_data$votechoice == "Liberal Party", 1,
                                                  ifelse(train_data$votechoice == "NDP", 1, 
                                                         2)))))

# Train Multinomial Logistic Regression model with class weights
log_model_2 <- train(votechoice ~ cps21_fed_gov_sat + pes21_province + pes21_inequal + pes21_abort2 + 
                   cps21_bornin_canada + cps21_marital + cps21_age, 
                   data = train_data_2,
                   method = "multinom",  
                   trControl = trainControl(method = "cv", number = 10, verboseIter = FALSE), 
                   weights = train_data_2$weights)

```

### Evaluation of Adjusted Model Performance (Second Prediction)

```{r}
# Use Multinomial Logistic Regression to predict
log_predictions_2 <- predict(log_model_2, newdata = test_data)

conf_matrix_log <- confusionMatrix(log_predictions_2, test_data$votechoice)


# Use Random Forest to predict
rf_predictions_2 <- predict(rf_model_2, newdata = test_data)

conf_matrix_rf <- confusionMatrix(rf_predictions_2, test_data$votechoice)

# Print Result
cat("\n---", "log_predictions_2", "---\n")
print(conf_matrix_log)
cat("\n---", "rf_predictions_2", "---\n")
print(conf_matrix_rf)
```

*Multinomial Logistic Regression*: 

Adjusting class weights led to an increase in performance for the Bloc Québécois and Conservative Party classes, but the Green Party and People's Party continue to be problematic with low sensitivity scores. There seems to be overfitting with the dominant classes, while the minority classes are still underrepresented.

*Random Forest*:

After adjusting the class weights, the accuracy dropped significantly from 64.32% to 52.48%. While the model's sensitivity for Bloc Québécois and Conservative Party improved, it failed to improve prediction performance for the minority classes, such as Green Party and People's Party, which continued to have low sensitivity values.

The Green Party and People's Party remain underrepresented, which suggests that further improvements are needed for minority class predictions, indicating a need for further refinement or use of additional techniques (e.g., SMOTE).


```{r}

```

