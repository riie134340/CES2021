---
title: "Modeling - xgboost"
output:
  html_document:
    df_print: paged
---

### Load Data

We loaded the pre-processed data saved from the previous step.
The initial dataset contained a total of 15,484 rows, with several variables related to voters' characteristics and political opinions.

```{r load-libs, message=FALSE}
rm(list = ls())
library(dplyr)
library(xgboost)
library(caret)

#load("preprocessed_data.RData")
load("scripts/preprocessed_data_2.RData")
```
### check new feature variable

```{r include=FALSE}
# Check the structure of the main data
colnames(ces_Modeling)

ces_Modeling <- ces_Modeling %>%
  select(-imm_duration, -cps21_rel_imp, -Duration__in_seconds_)

add_vars <- c("pes21_paymed", "pes21_senate", "pes21_losetouch", "pes21_hatespeech", 
              "pes21_envirojob", "pes21_govtcare", "pes21_famvalues", "pes21_bilingualism",
              "pes21_equalrights","pes21_fitin","pes21_immigjobs","pes21_foreign", "pes21_emb_satif",
              "pes21_donerm", "pes21_privjobs", "pes21_stdofliving", "pes21_trust", "pes21_newerlife")
feature_cols <- names(ces_Modeling)

duplicated_cols <- intersect(add_vars, feature_cols)
duplicated_cols

length(add_vars)
length(duplicated_cols)
```

```{r include=FALSE}
na_counts <- sapply(ces_Modeling, function(x) sum(is.na(x)))
high_na_cols <- names(na_counts[na_counts > 4000])
na_counts[high_na_cols]


duplicated_cols <- intersect(add_vars, high_na_cols)
duplicated_cols

length(duplicated_cols)
```
```{r}
modeling_var_clean <- ces_Modeling %>%
  select(-all_of(high_na_cols))
#modeling_var_clean <- ces_Modeling[, na_counts <= 9000]

# Remove rows with NA values in any of the features or target
# modeling_var_clean <- modeling_var_clean %>% na.omit()
summary(modeling_var_clean)
```
### Split Data

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(modeling_var_clean)), size = 0.7 * nrow(modeling_var_clean))
train_data <- modeling_var_clean[train_index, ]
test_data <- modeling_var_clean[-train_index, ]
```


### XGBoost - Data Preparation

```{r}
# Convert factor columns to numeric (using your original approach)
train_data_numeric <- train_data %>% mutate_if(is.factor, as.numeric)
test_data_numeric  <- test_data %>% mutate_if(is.factor, as.numeric)

# Define feature columns (exclude 'votechoice')
features <- setdiff(names(train_data_numeric), "votechoice")

# Create feature matrices for train and test
train_matrix <- as.matrix(train_data_numeric[, features])
test_matrix  <- as.matrix(test_data_numeric[, features])

# Create labels
train_label <- as.numeric(train_data_numeric$votechoice) - 1
test_label  <- as.numeric(test_data_numeric$votechoice) - 1

# Compute sample weights
class_counts <- table(train_data_numeric$votechoice)
total_count  <- sum(class_counts)
n_class      <- length(class_counts)

# Class weight = total / (num_classes * class_frequency)
class_weights_map <- total_count / (n_class * class_counts)
sample_weights <- class_weights_map[ as.character(train_data_numeric$votechoice) ]

class_weights_map

# Build XGBoost DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label, weight = sample_weights)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_label)
```

### XGBoost - Training

```{r}
train_and_evaluate <- function(params, dtrain, dtest, nrounds = 100) {
  # Train model
  model <- xgb.train(
    params  = params,
    data    = dtrain,
    nrounds = nrounds,
    verbose = 0
  )
  
  # Predict probabilities
  pred_prob <- predict(model, newdata = dtest)
  
  # Convert to matrix and get class with max probability
  pred_mat  <- matrix(pred_prob, ncol = n_class, byrow = TRUE)
  pred_idx  <- max.col(pred_mat) - 1
  
  # Convert back to factor labels
  pred_labels <- factor(pred_idx, levels = 0:(n_class-1), labels = levels(train_data$votechoice))
  
  # Generate confusion matrix
  cm <- confusionMatrix(pred_labels, test_data$votechoice)
  
  return(list(model = model, pred = pred_labels, cm = cm))
}
```

```{r}
params_1 <- list(
  objective        = "multi:softprob",  # Multi-class probability output
  num_class        = n_class,
  eval_metric      = "mlogloss",        # Multi-class log loss
  eta              = 0.1,               # Learning rate
  max_depth        = 6,                 # Max tree depth
  min_child_weight = 1,                 # Min child weight
  subsample        = 0.8,               # Subsample ratio
  colsample_bytree = 0.8,               # Column sample by tree
  gamma            = 0                  # Min loss reduction for split
)
pred_1 <- train_and_evaluate(params_1, dtrain, dtest, nrounds = 100)
pred_1$cm
```


```{r}
importance_matrix <- xgb.importance(
  feature_names = colnames(train_matrix),
  model = pred_1$model
)
print(importance_matrix)
xgb.plot.importance(importance_matrix[1:20,])
```

### Adjust XGBoost parameters

```{r}
params_2 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 6,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_2 <- train_and_evaluate(params_2, dtrain, dtest, nrounds = 100)
pred_2$cm

params_3 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_3 <- train_and_evaluate(params_3, dtrain, dtest, nrounds = 100)
pred_3$cm

```

We performed automatic hyperparameter tuning using 5-fold cross-validation.
The process included aggregating the cross-validation results and selecting the best set of parameters based on model performance.

```{r eval=FALSE}
ctrl <- trainControl(
  method = "cv", 
  number = 5, 
  verboseIter = TRUE
)

tune_grid <- expand.grid(
  nrounds = c(100, 200, 500),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.7, 0.8)
)

set.seed(42)
xgb_tuned <- train(
  x = train_matrix,
  y = factor(train_label), 
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = tune_grid,
  metric = "Accuracy"
)

print(xgb_tuned$bestTune)

```

The final output was:

```
Aggregating results
Selecting tuning parameters
Fitting nrounds = 200, max_depth = 4, eta = 0.05, gamma = 0, colsample_bytree = 0.7, min_child_weight = 1, subsample = 0.7 on full training set
```


```{r}
params_4 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_4 <- train_and_evaluate(params_4, dtrain, dtest, nrounds = 200)
pred_4$cm
```

```{r}

params_5 <- params_4
set.seed(42)
cv_result <- xgb.cv(
  params                = params_5,
  data                  = dtrain,
  nrounds               = 500,                
  nfold                 = 5,
  early_stopping_rounds = 50,
  verbose               = 1,
  stratified            = TRUE,               
  showsd                = TRUE,
  metrics               = "mlogloss"
)

best_nrounds <- cv_result$best_iteration
cat("Best nrounds from CV:", best_nrounds, "\n")


pred_5 <- train_and_evaluate(params_5, dtrain, dtest, nrounds = best_nrounds)
pred_5$cm
```


