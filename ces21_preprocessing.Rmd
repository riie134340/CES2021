---
title: "Correlation test - updated"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

### Imported dataset

```{r setup, include=FALSE}
rm(list = ls())

library(haven)
library(tidyverse)

ces2021 <- read_dta("dataverse_files/2021 Canadian Election Study v2.0.dta")
```


### Selecting variables

Before building any predictive model, we begin by selecting a set of variables to explore their relationships with the target variable (`pes21_votechoice2021`), as well as among themselves.

- `feature_vars` contains variables potentially relevant predictors we need to explore.

In addition, two variable sets are included for diagnostic purposes:
- `check_disengaged`: used to identify politically disengaged respondents.
- `check_low_quality`: used to flag low-quality or problematic responses (e.g., duplicates, speeders, inattentiveness).

We combine all of these into a single dataframe (`ces_selected`) to conduct correlation testing in the next step.

```{r}
target_var <- "pes21_votechoice2021"
mia_vars <- c("pes21_province", "cps21_age", "pes21_follow_pol", "pes21_rural_urban", 
              "pes21_inequal", "pes21_abort2", "pes21_contact1", "Region", "cps21_marital",
              "cps21_imm_year", "cps21_bornin_canada", "cps21_rel_imp", "cps21_volunteer")

extra_vars <- c("cps21_education","pes21_lived", "cps21_fed_gov_sat", "Duration__in_seconds_")

# Merge selected variables
feature_vars <- unique(c(mia_vars, extra_vars))


# use for check data quality
check_disengaged <- c("pes21_follow_pol", "cps21_interest_gen_1", "cps21_interest_elxn_1",
                      "cps21_news_cons","cps21_govt_confusing")

check_low_quality <- c("cps21_duplicates_pid_flag", "cps21_duplicate_ip_demo_flag", 
                       "pes21_speeder_low_quality","pes21_duplicates_pid_flag",
                       "cps21_inattentive","pes21_inattentive")

selected_var <- unique(c(target_var, mia_vars, extra_vars, check_disengaged, check_low_quality))
ces_selected <- ces2021 %>% select(all_of(selected_var))
head(ces_selected)
```

### Creating disengagement and data quality flags

#### identify disengagement group

To define political disengagement, the following survey items are used:

*pes21_follow_pol* And how closely do you follow politics on TV, radio, newspapers, or the Internet?

- Very closely (1)
- Fairly closely (2)
- Not very closely (3)
- Not at all (4)

*cps21_interest_gen_1* How interested are you in politics generally? Set the slider to a number from 0 to 10, where 0 means no interest at all, and 10 means a great deal of interest.

*cps21_interest_elxn_1* How interested are you in this federal election? Set the slider to a number from 0 to 10, where 0 means no interest at all, and 10 means a great deal of interest.

*cps21_news_cons* On average, how much time do you usually spend watching, reading, and listening to news each day?

- None (1)
- 1-10 minutes (2)
- 11-30 minutes (3)
- 31-60 minutes (4)
- Between 1 and 2 hours (5)
- More than 2 hours (6)
- Don't know/ Prefer not to answer (7)

*cps21_govt_confusing* Sometimes, politics and government seem so complicated that a person like me can't really understand what's going on.

- Strongly disagree (1)
- Somewhat disagree (2)
- Somewhat agree (3)
- Strongly agree (4)
- Don't know/ Prefer not to answer (5)

These variables are combined into a simple count (`disengaged_count`) to reflect the number of disengagement indicators present for each respondent.

```{r}
# Each component reflects low political interest, low media engagement, or confusion.
# Missing values (NA) are treated as disengaged (i.e., score = 1),
# since nonresponse may reflect a lack of political interest or attentiveness.

ces_selected$disengaged_count <- with(ces_selected,
  as.integer(is.na(pes21_follow_pol)      | pes21_follow_pol >= 3) +
  as.integer(is.na(cps21_interest_gen_1)  | cps21_interest_gen_1 <= 2) +
  as.integer(is.na(cps21_interest_elxn_1) | cps21_interest_elxn_1 <= 2) +
  as.integer(is.na(cps21_news_cons)       | cps21_news_cons %in% c(1, 7)) +
  as.integer(is.na(cps21_govt_confusing)  | cps21_govt_confusing == 5)
)

print("Distribution of disengaged_count:")
table(ces_selected$disengaged_count)
```
We define respondents with disengaged_count ≥ 3 as politically disengaged.
This threshold reflects a combination of at least 3 disengagement indicators,and captures roughly 8% of the sample.

```{r}
disengaged_threshold <- 3
```

#### identify low quality group

According to the original codebook, a number of severe data quality issues (e.g., incomplete responses, failed attention checks, straightlining) were already removed from the public dataset.

However, some respondents were flagged for less-severe issues and retained. These include:
- Inattentive respondents (e.g., those taking unusually long to complete the survey)
- Duplicate IP/demo matches
- Initial duplicates
- PES speeders (respondents who completed the post-election survey unusually fast)

We use the following variables to track these lower-level quality concerns:
- `cps21_duplicates_pid_flag`
- `cps21_duplicate_ip_demo_flag`
- `cps21_inattentive`
- `pes21_speeder_low_quality`
- `pes21_duplicates_pid_flag`
- `pes21_inattentive`

To simplify later filtering or robustness checks, we create a `low_quality_count` variable to count how many of these flags are triggered per respondent.

```{r}
# Compute a low_quality_count score to summarize how many data quality flags each respondent triggered. 
# Each variable is binary (0 = no issue, 1 = issue).
# Missing values (NA) are treated as 0 (i.e., no issue), to avoid excluding respondents.

ces_selected$low_quality_count <- rowSums(ces_selected[check_low_quality], na.rm = TRUE)

print("Distribution of low_quality_count:")
table(ces_selected$low_quality_count)
```
Based on this distribution, we define low_quality_count ≥ 3 as low quality.

Note: This is the first round of cleaning. 
A second round of filtering based on survey duration (e.g., too fast or too slow) will be applied later.


```{r}
low_quality_threshold <- 3
```

#### remove unnecessary variables

We convert labelled variables to readable factor levels using `as_factor()`, making the data easier to interpret and use in further exploring.

```{r}
# convert to readable entry
ces_selected_converted <- ces_selected %>% mutate(across(where(is.labelled), as_factor))

# remove the variables for checking quality
ces_feature <- ces_selected_converted %>% 
  select(all_of(c(target_var, feature_vars)), disengaged_count, low_quality_count)

head(ces_feature)
```

Variables used strictly for quality checks (e.g., duplicate flags) are removed from the main feature set,  
but `disengaged_count` and the raw quality flags are retained for possible use in filtering or exploratory analysis. 

### Data Cleaning

Before modeling, we examine the distribution and potential correlations of these features to decide whether they should be included in the model.

Here is the code used to inspect the unique data entries for each selected feature.
By reviewing these values, we can identify issues like missing data, inconsistent formatting,or unexpected categories — which indicates that data cleaning is needed before analysis.

```{r}
# return unique entry for each feature
get_feature_levels <- function(data, column_name) {
  if (!column_name %in% names(data)) {
    stop("Column not found in dataset.")
  }
  unique_values <- unique(data[[column_name]])
  return(unique_values)
}

for (var in feature_vars) {
  # Skip only "duration__in_seconds_"
  if (var == "duration__in_seconds_") next
  
  cat("\n---", var, "---\n")
  print(get_feature_levels(ces_feature, var))
}
```
We are beginning with handling ambiguous responses such as NA and "don't know". In parallel, we aim to identify patterns of political apathy, which may be reflected through missing values, neutral responses, or lack of engagement.

```{r}
replace_dontknow_with_na <- function(col) {
  if (is.character(col) || is.factor(col)) {
    col <- as.character(col)
    col[grepl("don.?t\\s*know|prefer not to answer", col, ignore.case = TRUE)] <- NA
    return(as.factor(col))
  } else {
    return(col)
  }
}

# Apply the function to all columns
ces_feature <- ces_feature %>% mutate(across(everything(), replace_dontknow_with_na))
```

### Handling disengaged data

```{r}
# Since We define respondents with disengaged_count ≥ 3 as politically disengaged
# Separate low-quality respondents based on low_quality_count.
low_quality <- ces_feature %>%
  filter(disengaged_count >= disengaged_threshold)

ces_feature_cleaned <- ces_feature %>%
  filter(disengaged_count < disengaged_threshold)
```

### Handling low-quality data

Based on survey duration time, we identified some responses as unreliable.
These cases are also labeled as politically disengaged.
Since they may bias the model, we temporarily remove them from the dataset before modeling.

```{r}
library(ggplot2)
ggplot(ces_feature_cleaned, aes(x = Duration__in_seconds_)) +
  geom_histogram(binwidth = 10) +
  xlim(0, 2400) +  
  labs(title = "Distribution of Survey Duration",
       x = "Duration (seconds)", y = "Count")

summary(ces_feature$Duration__in_seconds_)
```
The summary statistics of the `Duration` variable are as follows:

- **Minimum**: 362 seconds (~6 minutes)  
- **1st Quartile (Q1)**: 995 seconds (~16.6 minutes)  
- **Median**: 1325 seconds (~22 minutes)  
- **Mean**: 8710 seconds (significantly inflated by outliers)  
- **3rd Quartile (Q3)**: 1875 seconds (~31.3 minutes)  
- **Maximum**: 1,575,155 seconds (> 400 hours)

According to the summary, These values suggest that while most respondents completed the survey in under 30 minutes, there are a few extreme outliers with excessively long durations that strongly distort the mean.

The **minimum value of 362 seconds** and the **Q1 value of 995 seconds** suggest that any respondent completing the survey in under 10 minutes may not have engaged meaningfully with the content. Similarly, values above 48 hour are highly suspicious and may indicate participants who were inactive for long periods.

Therefore, a threshold of **600 seconds (10 minutes)** was chosen to identify "too fast" respondents, while an upper cap of **172800 seconds (48 hour)** was applied to identify "too slow" responses.

```{r}
filter_by_duration <- function(data, duration_col = "Duration__in_seconds_", 
                               fast_threshold = 600, slow_threshold = 172800) {
  data %>%
    filter(
      .data[[duration_col]] >= fast_threshold,
      .data[[duration_col]] <= slow_threshold
    )
}
```

```{r}
ces_feature_cleaned <- filter_by_duration(ces_feature_cleaned)
```

Then Remove responses considered low quality:
We exclude any row where 'low_quality_count' is greater than low_quality_threshold
This helps reduce noise from unreliable responses (e.g., inconsistent answers or other quality issues).

```{r}
ces_feature_cleaned <- ces_feature_cleaned %>% filter(low_quality_count <= low_quality_threshold)
```

### Pre-step for Correlation Test

Since these features are of different types and most of them are non-numeric. We cannot apply a single, unified statistical method. 
Instead, we need to adopt different analysis strategies based on the nature of each variable.

Next, we divide the selected features into two groups based on their data types:
- Categorical features will be tested using Cramér's V
- Ordinal or continuous features will be tested using the Kruskal-Wallis test
This helps us evaluate the strength of correlation between each feature and the target variable.

```{r}
# Initialize lists for variable classification
list_chi <- c()
list_kruskal <- c()

# Target variable (e.g. party vote choice)
target <- ces_feature_cleaned[[target_var]]
is_target_cat <- is.factor(target) || is.character(target)

# Loop through feature variables
if (is_target_cat) {
  for (var in feature_vars) {
    x <- ces_feature_cleaned[[var]]
    
    if (is.factor(x) || is.character(x)) {
      list_chi <- c(list_chi, var)
    } else if (is.numeric(x) || is.ordered(x)) {
      list_kruskal <- c(list_kruskal, var)
    }
  }
}

# Print results
cat("Variables for Cramér's V (categorical):\n")
print(list_chi)

cat("\nVariables for Kruskal-Wallis (numeric or ordered):\n")
print(list_kruskal)
```
### Correlation Test

For the features in list_chi, we compute their correlation with the target variable (party vote choice) using Cramér's V. The calculation uses the cramerV() function from the rcompanion package, which automatically removes observations with missing values (NA).


```{r}
library(rcompanion)  # cramerV
cramer_results <- data.frame(Variable = character(),
                             CramersV = numeric(),
                             stringsAsFactors = FALSE)

for (var in list_chi) {
  tbl <- table(ces_feature_cleaned[[target_var]], ces_feature_cleaned[[var]])
  if (min(dim(tbl)) > 1) {
    result <- cramerV(tbl, bias.correct = TRUE)
    cramer_results <- rbind(cramer_results, data.frame(Variable = var, CramersV = result))
  }
}

# print result
cramer_results <- cramer_results[order(-cramer_results$CramersV), ]
print(cramer_results, row.names = FALSE)

```

Higher Cramér's V values indicate stronger associations with the target variable.
Variables such as Region and Province showed relatively strong correlations with vote choice, while others like Volunteer activity and Immigration year had weaker or missing correlations.

During the Cramér's V analysis, we found that 'cps21_imm_year' returned NaN. It might because the 'cps21_imm_year' variable has many unique values (immigration years).
To address this, we converted 'cps21_imm_year' into 'years since immigration' by subtracting it from 2021.
This transformed variable is numeric and can be meaningfully analyzed using the Kruskal–Wallis test.

```{r}
# convert new variable
ces_feature_cleaned$imm_duration <- 2021 - as.numeric(ces_feature_cleaned$cps21_imm_year)

# add to list_kruskal
list_kruskal <- c(list_kruskal, "imm_duration")
```


Then we applied the Kruskal–Wallis test to evaluate whether the distributions of features in list_kruskal differ significantly across vote choice categories.

```{r}
kruskal_results <- data.frame(Variable = character(),
                              KruskalP = numeric(),
                              stringsAsFactors = FALSE)

for (var in list_kruskal) {
  df <- na.omit(ces_feature_cleaned[, c(var, target_var)])
  formula <- as.formula(paste(var, "~", target_var))
  result <- kruskal.test(formula, data = df)
  
  kruskal_results <- rbind(kruskal_results,
                           data.frame(Variable = var,
                                      KruskalP = result$p.value))
}

# print result
kruskal_results <- kruskal_results[order(kruskal_results$KruskalP), ]
print(kruskal_results)

```

Since small p-values indicate strong evidence of differences between groups.
The features 'cps21_age', 'Duration__in_seconds_', and 'imm_duration' all represented that these features are highly associated with voting behavior and may be valuable for predictive modeling.

We will use the following features in the prediction model:
```{r}
# filter variables with Cramér's V > 0.1
selected_cramer_vars <- cramer_results %>%
  filter(CramersV > 0.1) %>%
  pull(Variable)

# filter variables with kruskal < 0.05
selected_kruskal_vars <- kruskal_results %>%
  filter(KruskalP < 0.05) %>%
  pull(Variable)

selected_model_vars <- unique(c(selected_cramer_vars, selected_kruskal_vars))
print(selected_model_vars)
```

### Checking Feature Redundancy

To prevent multicollinearity in the model, we calculated pairwise Cramér’s V scores among features to identify strongly correlated variables.

Interpretation thresholds:
• V > 0.6 — High correlation: likely redundant; consider removing one of the variables.
• V > 0.4 — Moderate correlation: possible redundancy; proceed with caution.
• V < 0.3 — Low correlation: safe to include both variables.


```{r}
library(rcompanion)

feature_corr_results <- data.frame(VarA = character(),
                                   VarB = character(),
                                   CramersV = numeric(),
                                   stringsAsFactors = FALSE)

for (i in 1:(length(selected_cramer_vars)-1)) {
  for (j in (i+1):length(selected_cramer_vars)) {
    varA <- selected_cramer_vars[i]
    varB <- selected_cramer_vars[j]

    clean_data <- ces_feature_cleaned %>%
      dplyr::select(all_of(c(varA, varB))) %>%
      dplyr::filter(!is.na(.data[[varA]]), !is.na(.data[[varB]]))

    tbl <- table(clean_data[[varA]], clean_data[[varB]])

    if (min(dim(tbl)) > 1) {
      result <- cramerV(tbl, bias.correct = TRUE)
      feature_corr_results <- rbind(feature_corr_results,
                                    data.frame(VarA = varA, VarB = varB, CramersV = result))
    }
  }
}

# print out
feature_corr_results <- feature_corr_results %>%
  mutate(Explanation = case_when(
    CramersV > 0.6 ~ "High correlation – consider removing one variable",
    CramersV > 0.4 ~ "Moderate correlation – possible redundancy",
    TRUE ~ "Low correlation – likely safe to include both"
  ))

feature_corr_results <- feature_corr_results[order(-feature_corr_results$CramersV), ]
print(feature_corr_results)
```


Among all feature pairs, only `Region` and `pes21_province` showed a high Cramér’s V (0.9997), indicating near-perfect redundancy. Since both represent geographic information.
We will retain only one of them to avoid duplication. In this case, we choose to keep `pes21_province`.

### Result

Update the list of features in the prediction model:
```{r}
selected_cramer_vars <- setdiff(selected_cramer_vars, "Region")
selected_model_vars <- unique(c(selected_cramer_vars, selected_kruskal_vars))
print(selected_model_vars)
```